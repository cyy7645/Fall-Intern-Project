{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 导入库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.metrics import accuracy_score, log_loss\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.metrics import accuracy_score, log_loss\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy import stats\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import math\n",
    "from scipy import stats\n",
    "from sklearn.utils.multiclass import type_of_target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 对数据集自动清理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "预定义函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 删除重复列\n",
    "def drop_dup_cols(input_dataframe):\n",
    "    input_dataframe.T.drop_duplicates().T\n",
    "    return input_dataframe\n",
    "\n",
    "# 删除含有空值的行\n",
    "def drop_nans(input_dataframe):\n",
    "    input_dataframe.dropna(inplace=True)\n",
    "    return input_dataframe\n",
    "\n",
    "# 删除指定列\n",
    "def drop_col(input_dataframe, column):\n",
    "    input_dataframe = input_dataframe.drop([column],axis=1)\n",
    "    print(\"成功删除特征：\",column)\n",
    "    return input_dataframe\n",
    "\n",
    "# 删除异常值\n",
    "def drop_outfiers(input_dataframe, column):\n",
    "    drop_indices = []\n",
    "    percent_25 = np.percentile(input_dataframe[column],25)\n",
    "    percent_75 = np.percentile(input_dataframe[column],75)\n",
    "    normal_range = (percent_75 - percent_25) * 3\n",
    "    outlier_indexs = input_dataframe[(input_dataframe[column] < percent_25 - normal_range) | (input_dataframe[column] > percent_75 + normal_range)].index\n",
    "    drop_indices.extend(list(outlier_indexs ))\n",
    "    input_dataframe = input_dataframe.drop(drop_indices)\n",
    "    return input_dataframe, percent_25, percent_75\n",
    "\n",
    "# 查找指定比例的值作为异常值，用极大极小值代替\n",
    "def replace_outfiers_by_percent(input_dataframe, column, percent):\n",
    "    drop_indices = []\n",
    "    percent_min = np.percentile(input_dataframe[column],100-percent)\n",
    "    percent_max = np.percentile(input_dataframe[column],percent)\n",
    "    input_dataframe[column] = input_dataframe[column].map(lambda x: percent_max if x > percent_max else x)\n",
    "    input_dataframe[column] = input_dataframe[column].map(lambda x: percent_min if x < percent_min else x)\n",
    "    return input_dataframe,percent_min,percent_max\n",
    "\n",
    "# 对指定特征使用one hot 编码\n",
    "def one_hot_encoder_feature(input_dataframe, column):\n",
    "    one_hot_column = pd.get_dummies(input_dataframe[column])\n",
    "    input_dataframe = pd.concat([one_hot_column,input_dataframe],axis=1)\n",
    "    input_dataframe = input_dataframe.drop([column],axis=1)\n",
    "    return input_dataframe\n",
    "\n",
    "# 对指定特征使用label encoder 编码\n",
    "def label_encoder(input_dataframe, column):\n",
    "    column_encoder = LabelEncoder().fit(input_dataframe[column].values)\n",
    "    input_dataframe[column] = column_encoder.transform(input_dataframe[column].values)\n",
    "    return input_dataframe\n",
    "\n",
    "# 对特征归一化处理\n",
    "def normalize(input_dataframe, column, norm_max, norm_min):\n",
    "    '''\n",
    "    norm_min: int\n",
    "        归一化后范围的下界 （默认：0）\n",
    "    norm_max: int\n",
    "        归一化后范围的上界 （默认：1）\n",
    "    '''\n",
    "    min_x = min(input_dataframe[column])\n",
    "    max_x = max(input_dataframe[column])\n",
    "    range_x = max_x - min_x\n",
    "    range_norm = norm_max - norm_min\n",
    "    input_dataframe[column] = input_dataframe[column].map(lambda x: x/range_x * range_norm)\n",
    "    return input_dataframe, min_x, max_x\n",
    "\n",
    "# 对空值进行处理\n",
    "def deal_non(input_dataframe, column, null_method, similarity_features, remain_features):\n",
    "    if null_method:\n",
    "        if null_method not in ['mean','mode','median','similarity']:\n",
    "            print(\"请输入正确的空缺值处理方法\")\n",
    "            return\n",
    "\n",
    "    # 若指定以平均数替代\n",
    "    if null_method == 'mean':\n",
    "        replace_value = input_dataframe[column].mean()\n",
    "        input_dataframe[column].fillna(replace_value, inplace=True)\n",
    "\n",
    "    # 若指定以中位数替代\n",
    "    elif null_method == 'median':\n",
    "        replace_value = input_dataframe[column].median()\n",
    "        input_dataframe[column].fillna(replace_value, inplace=True)\n",
    "\n",
    "    # 若指定以众数替代\n",
    "    elif null_method == 'mode':\n",
    "        replace_value = input_dataframe[column].mode()\n",
    "        input_dataframe[column].fillna(input_dataframe[column].mode(), inplace=True)\n",
    "\n",
    "    # 若指定以相似度替代\n",
    "    elif null_method == 'similarity':\n",
    "        if not similarity_features:\n",
    "            print(\"请输入相似度参考特征\")\n",
    "            return\n",
    "\n",
    "        # 输入的特征不属于原特征\n",
    "        elif set(remain_features) > set(similarity_features) == False:\n",
    "            print(\"请输入正确的特征名\")\n",
    "\n",
    "        # 根据提供特征的相似度替换空缺值\n",
    "        else:\n",
    "            if column not in similarity_features and input_dataframe[column].dtype == 'int64' or input_dataframe[column].dtype == 'float64':\n",
    "                input_dataframe[column].fillna(data.groupby(similarity_features)[column].transform('mean'),inplace=True)\n",
    "            else:\n",
    "                input_dataframe[column].fillna(input_dataframe[column].mode(), inplace=True)\n",
    "    return input_dataframe, replace_value\n",
    "\n",
    "# 识别年份，特殊处理，转换为object类型，之后转换为枚举类\n",
    "def detect_year(input_dataframe, column):\n",
    "    tmp_year = column.lower()\n",
    "    pattern = r\"year\"\n",
    "    if re.search(pattern,tmp_year):\n",
    "        input_dataframe[column] = input_dataframe[column].astype('object')\n",
    "    return input_dataframe\n",
    "\n",
    "# 识别编号，特殊处理\n",
    "def detect_number(input_dataframe, column):\n",
    "    tmp_no = column.lower()\n",
    "    pattern = r\"num|no|number\"\n",
    "    if re.search(pattern,tmp_no):\n",
    "        input_dataframe[column] = input_dataframe[column].astype('str')\n",
    "    return input_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def autoclean(input_dataframe, normalization = False, drop_nans=False,\n",
    "              nans_convert_binary = False, null_method = 'mode', \n",
    "              similarity_features = None, enum_continuous = False, \n",
    "              one_hot_encoder = False, no_one_hot_features = None, \n",
    "              norm_max = 1, norm_min = 0, \n",
    "              replace_outfiers_by_percent = False, percent = 95):\n",
    "    \"\"\"对数据集进行标准化数据预处理\n",
    "    \n",
    "    ----------\n",
    "    input_dataframe: pandas.DataFrame\n",
    "        输入的数据集\n",
    "    normalization: bool\n",
    "        对数据归一化 （默认: False）\n",
    "    drop_nans: bool\n",
    "        删除所有包含NaN的行 (默认: False)\n",
    "    nans_convert_binary: bool\n",
    "        把存在的值替换为1，不存在替换为0\n",
    "    null_method: string\n",
    "        空缺值处理方法 （默认: mode）\n",
    "        可选 mode, average, median, similarity\n",
    "    similarity_features: list\n",
    "        以list中的特征作为相似度处理空缺值\n",
    "    enum_continuous: bool\n",
    "        把连续型变量转换为枚举类 （默认: True）\n",
    "    one_hot_encoder: bool\n",
    "        使用OneHotEncoder编码方式. (默认: False)\n",
    "    no_one_hot_features: list\n",
    "        指定不需要使用OneHotEncoder编码的特征 (默认：None)\n",
    "    norm_min: int\n",
    "        归一化后范围的下界 （默认：0）\n",
    "    norm_max: int\n",
    "        归一化后范围的上界 （默认：1）\n",
    "    replace_outfiers_by_percent: bool\n",
    "        用极大极小值代替指定头尾百分比的值 （默认：False）\n",
    "    percent: float\n",
    "        仅当replace_outfiers_by_percent为True时有效，前percent百分比\n",
    "        和后（100-percent）百分比的值被认为异常值\n",
    "    Returns\n",
    "    ----------\n",
    "    input_dataframe: pandas.DataFrame\n",
    "        清理完成后的数据集\n",
    "    \"\"\"\n",
    "    # 剩下的特征名\n",
    "    remain_features = input_dataframe.columns.tolist()\n",
    "        \n",
    "    # 删除重复的特征\n",
    "    input_dataframe = drop_dup_cols(input_dataframe)\n",
    "    \n",
    "    # 删除包含NaN的行\n",
    "    if drop_nans:\n",
    "        input_dataframe = drop_nans(input_dataframe)\n",
    "        remain_features.remove(column)\n",
    "    \n",
    "    # 对于每一个特征\n",
    "    for column in input_dataframe.columns.values:\n",
    "        print(\"当前特征\", column)\n",
    "        \n",
    "        # 分别计算记录总条数，种类数，空值数，0的数量，唯一值比例，空值比例，值为0的比例\n",
    "        count = input_dataframe[column].count()\n",
    "        unique_sum = len(input_dataframe[column].unique())\n",
    "        empty = input_dataframe[column].isnull().sum()\n",
    "        zeros = (input_dataframe[column] == 0).sum()\n",
    "        unique_ratio = unique_sum / count\n",
    "        empty_ratio = empty / count\n",
    "        zeros_ratio = zeros/count\n",
    "        \n",
    "        # 判断是否为唯一标示特征，即当特征的每一列都不同，特征无效\n",
    "        if unique_ratio > 0.7:\n",
    "            input_dataframe = drop_col(input_dataframe, column)\n",
    "            remain_features.remove(column)\n",
    "            continue;\n",
    "            \n",
    "        # 当特征每一列都相同，特征无效\n",
    "        if unique_sum  == 1:\n",
    "            input_dataframe = drop_col(input_dataframe, column)\n",
    "            remain_features.remove(column)\n",
    "            continue;\n",
    "        \n",
    "        # 对于空值比例较高的特征（> 60%）\n",
    "        if empty_ratio > 0.6:\n",
    "            \n",
    "            # 如果参数nans_convert_binary为True, 把存在的值替换成1，空值为0\n",
    "            if nans_convert_binary == True:\n",
    "                input_dataframe[column] = input_dataframe[column].notnull().astype('int')\n",
    "            else:\n",
    "                # 删除特征\n",
    "                input_dataframe = drop_col(input_dataframe, column)\n",
    "                remain_features.remove(column)\n",
    "                continue;\n",
    "                \n",
    "        # 识别年份，特殊处理，转换为object类型，之后转换为枚举类\n",
    "        input_dataframe = detect_year(input_dataframe, column)\n",
    "            \n",
    "        # 识别编号，特殊处理\n",
    "        input_dataframe = detect_number(input_dataframe, column)\n",
    "        \n",
    "        # 如果存在空值需要对空值进行填补\n",
    "        if input_dataframe[column].isnull().values.any():\n",
    "            try:\n",
    "                print(remain_features)\n",
    "                if null_method != 'similarity':\n",
    "                    input_dataframe,_ = deal_non(input_dataframe, column, null_method, similarity_features,remain_features)\n",
    "                else:\n",
    "                    input_dataframe = deal_non(input_dataframe, column, null_method, similarity_features,remain_features)\n",
    "\n",
    "                unique = input_dataframe[column].unique()\n",
    "\n",
    "                # 特征值唯一为无效特征\n",
    "                if len(unique) == 1:\n",
    "                    input_dataframe = drop_col(input_dataframe, column)\n",
    "                    remain_features.remove(column)\n",
    "                    continue; \n",
    "\n",
    "                # 删除异常值\n",
    "                if str(input_dataframe[column].dtype) == 'int64' or str(input_dataframe[column].dtype) == 'float64':\n",
    "\n",
    "                    # 若特征包含大量0，会导致25分位数和75分位数为0，需要特殊处理               \n",
    "                    if zeros_ratio < 0.7:\n",
    "                        # 删除异常值\n",
    "                        if replace_outfiers_by_percent == False:\n",
    "                            input_dataframe,_,_ = drop_outfiers(input_dataframe, column)\n",
    "                            print(type(input_dataframe))\n",
    "                        # 用极大极小值代替异常值\n",
    "                        else:\n",
    "                            input_dataframe[column],_,_ = replace_outfiers_by_percent(input_dataframe, column, percent)\n",
    "\n",
    "\n",
    "            # 对于string类型的特征\n",
    "            except TypeError:\n",
    "                most_frequent = input_dataframe[column].mode()\n",
    "                if len(most_frequent) > 0:\n",
    "                    input_dataframe[column].fillna(input_dataframe[column].mode()[0], inplace=True)\n",
    "                else:\n",
    "                    input_dataframe[column].fillna(method='bfill', inplace=True)\n",
    "        \n",
    "        # 把所有string类型转换成枚举类型        \n",
    "        try:\n",
    "            input_dataframe[column].values.astype('float')\n",
    "\n",
    "        except:\n",
    "            # 对指定特征采用OneHot编码\n",
    "            if one_hot_encoder == True:\n",
    "                ret = list(set(input_dataframe.columns.tolist()).difference(set(no_one_hot_features)))\n",
    "                if ret:\n",
    "                    if column not in no_one_hot_features:\n",
    "                        input_dataframe = one_hot_encoder_feature(input_dataframe, column)\n",
    "                        \n",
    "                    # 对未指定的特征使用LabelEncoder编码方式\n",
    "                    else:\n",
    "                        \n",
    "                        input_dataframe[column] = label_encoder(input_dataframe, column)\n",
    "                \n",
    "                        \n",
    "            # 使用LabelEncoder编码方式\n",
    "            else:\n",
    "                input_dataframe[column] = label_encoder(input_dataframe, column)\n",
    "                \n",
    "        # 是否归一化\n",
    "        if normalization:\n",
    "            input_dataframe[column],_,_ = normalize(input_dataframe, column, norm_max, norm_min)\n",
    "    return input_dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def autoclean_cv(input_dataframe, test_dataframe, normalization = False, drop_nans=False,\n",
    "              nans_convert_binary = False, null_method = 'mode', \n",
    "              similarity_features = None, enum_continuous = False, \n",
    "              one_hot_encoder = False, no_one_hot_features = None, \n",
    "              norm_max = 1, norm_min = 0, \n",
    "              replace_outfiers_by_percent = False, percent = 95):\n",
    "    \"\"\"\n",
    "    当输入数据集包含训练集和测试集时，需要对测试集采用和训练集同样的处理标准\n",
    "    \n",
    "    ----------\n",
    "    input_dataframe: pandas.DataFrame\n",
    "        输入的数据集\n",
    "    test_dataframe: pandas.DataFrame\n",
    "        输入的测试集\n",
    "    normalization: bool\n",
    "        对数据归一化 （默认: False）\n",
    "    drop_nans: bool\n",
    "        删除所有包含NaN的行 (默认: False)\n",
    "    nans_convert_binary: bool\n",
    "        把存在的值替换为1，不存在替换为0\n",
    "    null_method: string\n",
    "        空缺值处理方法 （默认: mode）\n",
    "        可选 mode, average, median, similarity\n",
    "    similarity_features: list\n",
    "        以list中的特征作为相似度处理空缺值\n",
    "    enum_continuous: bool\n",
    "        把连续型变量转换为枚举类 （默认: True）\n",
    "    one_hot_encoder: bool\n",
    "        使用OneHotEncoder编码方式. (默认: False)\n",
    "    no_one_hot_features: list\n",
    "        指定不需要使用OneHotEncoder编码的特征 (默认：None)\n",
    "    norm_min: int\n",
    "        归一化后范围的下界 （默认：0）\n",
    "    norm_max: int\n",
    "        归一化后范围的上界 （默认：1）\n",
    "    replace_outfiers_by_percent: bool\n",
    "        用极大极小值代替指定头尾百分比的值 （默认：False）\n",
    "    percent: float\n",
    "        仅当replace_outfiers_by_percent为True时有效，前percent百分比\n",
    "        和后（100-percent）百分比的值被认为异常值\n",
    "    Returns\n",
    "    ----------\n",
    "    input_dataframe: pandas.DataFrame\n",
    "        清理完成后的数据集\n",
    "    \"\"\"\n",
    "    if set(input_dataframe.columns.values) != set(test_dataframe.columns.values):\n",
    "        raise ValueError('训练集特征和测试集特征必须相同')\n",
    "    \n",
    "    # 剩下的特征名\n",
    "    remain_features = input_dataframe.columns.tolist()\n",
    "    test_remain_features = test_dataframe.columns.tolist()\n",
    "        \n",
    "    # 删除重复的特征\n",
    "    input_dataframe = drop_dup_cols(input_dataframe)\n",
    "    test_dataframe = drop_dup_cols(test_dataframe)\n",
    "    \n",
    "    # 删除包含NaN的行\n",
    "    if drop_nans:\n",
    "        input_dataframe = drop_nans(input_dataframe)\n",
    "        test_dataframe = drop_nans(test_dataframe)\n",
    "    \n",
    "    # 对于每一个特征\n",
    "    for column in input_dataframe.columns.values:\n",
    "        print(\"当前特征\", column)\n",
    "        \n",
    "        # 分别计算记录总条数，种类数，空值数，0的数量，唯一值比例，空值比例，值为0的比例\n",
    "        count = input_dataframe[column].count()\n",
    "        unique_sum = len(input_dataframe[column].unique())\n",
    "        empty = input_dataframe[column].isnull().sum()\n",
    "        zeros = (input_dataframe[column] == 0).sum()\n",
    "        unique_ratio = unique_sum / count\n",
    "        empty_ratio = empty / count\n",
    "        zeros_ratio = zeros/count\n",
    "        \n",
    "        # 判断是否为唯一标示特征，即当特征的每一列都不同，特征无效\n",
    "        if unique_ratio > 0.7:\n",
    "            input_dataframe = drop_col(input_dataframe, column)\n",
    "            remain_features.remove(column)\n",
    "            # 若测试集中的此特征被删除，训练集也应当删除\n",
    "            test_dataframe = drop_col(test_dataframe, column)\n",
    "            test_remain_features.remove(column)\n",
    "            continue;\n",
    "            \n",
    "        # 当特征每一列都相同，特征无效\n",
    "        if unique_sum  == 1:\n",
    "            input_dataframe = drop_col(input_dataframe, column)\n",
    "            remain_features.remove(column)\n",
    "            test_dataframe = drop_col(test_dataframe, column)\n",
    "            test_remain_features.remove(column)\n",
    "            continue;\n",
    "        \n",
    "        # 对于空值比例较高的特征（> 60%）\n",
    "        if empty_ratio > 0.6:\n",
    "            \n",
    "            # 如果参数nans_convert_binary为True, 把存在的值替换成1，空值为0\n",
    "            if nans_convert_binary == True:\n",
    "                input_dataframe[column] = input_dataframe[column].notnull().astype('int')\n",
    "                test_dataframe[column] = test_dataframe[column].notnull().astype('int')\n",
    "            else:\n",
    "                # 删除特征\n",
    "                input_dataframe = drop_col(input_dataframe, column)\n",
    "                remain_features.remove(column)\n",
    "                test_dataframe = drop_col(test_dataframe, column)\n",
    "                test_remain_features.remove(column)\n",
    "                continue;\n",
    "                \n",
    "        # 识别年份，特殊处理，转换为object类型，之后转换为枚举类\n",
    "        input_dataframe = detect_year(input_dataframe, column)\n",
    "        test_dataframe = detect_year(test_dataframe, column)\n",
    "            \n",
    "        # 识别编号，特殊处理\n",
    "        input_dataframe = detect_number(input_dataframe, column)\n",
    "        test_dataframe = detect_number(test_dataframe, column)\n",
    "                      \n",
    "        # 空值填补\n",
    "        try:\n",
    "            input_dataframe,replace_value = deal_non(input_dataframe, column, null_method, similarity_features)\n",
    "            # 测试集应该用训练集的方法填补\n",
    "            # 当训练集拿同一个值填补时，测试集也应当用这个值\n",
    "            if null_method in ['mean','mode','median']:\n",
    "                test_dataframe[column].fillna(replace_value, inplace=True)\n",
    "            # 当训练集用相似性填补时，测试集可以用自身数据集的相似度填补\n",
    "            else:\n",
    "                if column not in similarity_features and input_dataframe[column].dtype == 'int64' or input_dataframe[column].dtype == 'float64':\n",
    "                    test_dataframe[column].fillna(data.groupby(similarity_features)[column].transform('mean'),inplace=True)\n",
    "                else:\n",
    "                    test_dataframe[column].fillna(input_dataframe[column].mode(), inplace=True)\n",
    "            \n",
    "            # 训练集和测试集中不同取值的个数\n",
    "            unique = input_dataframe[column].unique().tolist()\n",
    "            test_unique = test_dataframe[column].unique().tolist()\n",
    "            \n",
    "            # 特征值唯一为无效特征\n",
    "            if len(unique) == 1:\n",
    "                input_dataframe = drop_col(input_dataframe, column)\n",
    "                remain_features.remove(column)\n",
    "                test_dataframe = drop_col(test_dataframe, column)\n",
    "                test_remain_features.remove(column)\n",
    "                continue; \n",
    "                \n",
    "            # 删除异常值\n",
    "            if input_dataframe[column].dtype == 'int64' or input_dataframe[column].dtype == 'float64':\n",
    "                \n",
    "                # 若特征包含大量0，会导致25分位数和75分位数为0，需要特殊处理               \n",
    "                if zeros_ratio < 0.7:\n",
    "                    if replace_outfiers_by_percent == False:\n",
    "                        drop_indices = []\n",
    "                        input_dataframe, percent_25, percent_75 = drop_outfiers(input_dataframe, column)\n",
    "                        # 测试集应当按照训练集的标准删除异常值\n",
    "                        normal_range = (percent_75 - percent_25) * 3\n",
    "                        outlier_indexs = test_dataframe[(test_dataframe[column] < percent_25 - normal_range) | (test_dataframe[column] > percent_75 + normal_range)].index\n",
    "                        drop_indices.extend(list(outlier_indexs ))\n",
    "                        test_dataframe = test_dataframe.drop(drop_indices)\n",
    "                    else:\n",
    "                        input_dataframe = replace_outfiers_by_percent(input_dataframe, column, percent)\n",
    "                        # 测试集应当按照训练集的标准删除异常值\n",
    "                        test_dataframe[column] = test_dataframe[column].map(lambda x: percent_max if x > percent_max else x)\n",
    "                        test_dataframe[column] = test_dataframe[column].map(lambda x: percent_min if x < percent_min else x)\n",
    "                        \n",
    "                    \n",
    "        \n",
    "        # 对于string类型的特征\n",
    "        except TypeError:\n",
    "            # 当测试集中特征的出现了训练集中没有出现的取值时，应删掉\n",
    "            unique = input_dataframe[column].unique().tolist()\n",
    "            test_unique = test_dataframe[column].unique().tolist()\n",
    "            if len(test_unique) > len(unique):\n",
    "                test_dataframe = test_dataframe[test_dataframe[column].isin(unique)]\n",
    "                print('delete successfully')\n",
    "            \n",
    "            # string类型的特征用出现最多的字符填补，测试集应该使用填补测试集的的字符填补\n",
    "            most_frequent = input_dataframe[column].mode()\n",
    "            if len(most_frequent) > 0:\n",
    "                input_dataframe[column].fillna(input_dataframe[column].mode()[0], inplace=True)\n",
    "                test_dataframe[column].fillna(input_dataframe[column].mode()[0], inplace=True)\n",
    "            else:\n",
    "                input_dataframe[column].fillna(input_dataframe[column].mode(), inplace=True)\n",
    "                test_dataframe[column].fillna(input_dataframe[column].mode(), inplace=True)\n",
    "\n",
    "        \n",
    "        # 把所有string类型转换成枚举类型\n",
    "        if str(input_dataframe[column].values.dtype) == 'object':\n",
    "            # 对指定特征采用OneHot编码\n",
    "            # 训练集和测试集应当使用相同的编码规则，否则当训练集的变量中多于测试集时，同一种变量的编码结果会不同\n",
    "            if one_hot_encoder == True:\n",
    "                ret = list(set(input_dataframe.columns.tolist()).difference(set(no_one_hot_features)))\n",
    "                if ret:\n",
    "                    if column not in no_one_hot_features:\n",
    "                        input_dataframe = one_hot_encoder_feature(input_dataframe, column)\n",
    "                        test_dataframe = one_hot_encoder_feature(test_dataframe, column)\n",
    "                    # 对未指定的特征使用LabelEncoder编码方式\n",
    "                    else:\n",
    "                        le = LabelEncoder()\n",
    "                        le.fit(input_dataframe[column].values.ravel())\n",
    "                        keys = le.classes_\n",
    "                        values = le.transform(le.classes_)\n",
    "                        dictionary = dict(zip(keys, values))\n",
    "                        input_dataframe[column] = input_dataframe[column].map(lambda x: dictionary[x])\n",
    "                        test_dataframe[column] = test_dataframe[column].map(lambda x: dictionary[x])\n",
    "                \n",
    "                        \n",
    "            # 使用LabelEncoder编码方式，训练集和测试集应当使用相同的编码规则\n",
    "            else:\n",
    "                le = LabelEncoder()\n",
    "                le.fit(input_dataframe[column].values.ravel())\n",
    "                keys = le.classes_\n",
    "                values = le.transform(le.classes_)\n",
    "                dictionary = dict(zip(keys, values))\n",
    "                input_dataframe[column] = input_dataframe[column].map(lambda x: dictionary[x])\n",
    "                test_dataframe[column] = test_dataframe[column].map(lambda x: dictionary[x])\n",
    "\n",
    "                \n",
    "        # 归一化\n",
    "        if normalization:\n",
    "            # 训练集归一化时储存min_x, max_x\n",
    "            dict_norm = {}\n",
    "            input_dataframe, min_x, max_x = normalize(input_dataframe, column, norm_min, norm_max)\n",
    "            min_max_list = []\n",
    "            min_max_list.append(min(input_dataframe[column]))\n",
    "            min_max_list.append(max(input_dataframe[column]))\n",
    "            dict_norm[column] = min_max_list\n",
    "            # 对测试集采用相同的范围归一化\n",
    "            range_norm = norm_max - norm_min\n",
    "            range_x = dict_norm[column][1] - dict_norm[column][0]\n",
    "            test_dataframe[column] = test_dataframe[column].map(lambda x: x/range_x * range_norm)\n",
    "\n",
    "    return test_dataframe\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用Titanic数据集测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5 21171</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C85</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Heikkinen, Miss. Laina</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>STON/O2. 3101282</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113803</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>C123</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Allen, Mr. William Henry</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>373450</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Survived  Pclass  \\\n",
       "0            1         0       3   \n",
       "1            2         1       1   \n",
       "2            3         1       3   \n",
       "3            4         1       1   \n",
       "4            5         0       3   \n",
       "\n",
       "                                                Name     Sex   Age  SibSp  \\\n",
       "0                            Braund, Mr. Owen Harris    male  22.0      1   \n",
       "1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
       "2                             Heikkinen, Miss. Laina  female  26.0      0   \n",
       "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
       "4                           Allen, Mr. William Henry    male  35.0      0   \n",
       "\n",
       "   Parch            Ticket     Fare Cabin Embarked  \n",
       "0      0         A/5 21171   7.2500   NaN        S  \n",
       "1      0          PC 17599  71.2833   C85        C  \n",
       "2      0  STON/O2. 3101282   7.9250   NaN        S  \n",
       "3      0            113803  53.1000  C123        S  \n",
       "4      0            373450   8.0500   NaN        S  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_dataframe = pd.read_csv(\"train.csv\")\n",
    "input_dataframe.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对Sex特征使用OneHot编码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "当前特征 PassengerId\n",
      "成功删除特征： PassengerId\n",
      "当前特征 Survived\n",
      "当前特征 Pclass\n",
      "当前特征 Name\n",
      "成功删除特征： Name\n",
      "当前特征 Sex\n",
      "Sex 1111\n",
      "当前特征 Age\n",
      "['Survived', 'Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Ticket', 'Fare', 'Cabin', 'Embarked']\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "当前特征 SibSp\n",
      "当前特征 Parch\n",
      "当前特征 Ticket\n",
      "成功删除特征： Ticket\n",
      "当前特征 Fare\n",
      "当前特征 Cabin\n",
      "成功删除特征： Cabin\n",
      "当前特征 Embarked\n",
      "['Survived', 'Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked']\n",
      "Embarked 1111\n"
     ]
    }
   ],
   "source": [
    "one_hot = ['Sex']\n",
    "\n",
    "# 指定不适用OneHot编码的特征\n",
    "no_one_hot_features = list(set(input_dataframe.columns.tolist()).difference(set(one_hot)))\n",
    "\n",
    "cleaned_data = autoclean(input_dataframe,one_hot_encoder = True, no_one_hot_features = no_one_hot_features, \n",
    "                         null_method = 'mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>female</th>\n",
       "      <th>male</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   female  male  Survived  Pclass   Age  SibSp  Parch     Fare  Embarked\n",
       "0       0     1         0       3  22.0      1      0   7.2500       0.0\n",
       "1       1     0         1       1  38.0      1      0  71.2833       1.0\n",
       "2       1     0         1       3  26.0      0      0   7.9250       1.0\n",
       "3       1     0         1       1  35.0      1      0  53.1000       1.0\n",
       "4       0     1         0       3  35.0      0      0   8.0500       0.0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 自动清理完成的数据\n",
    "cleaned_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 使用卡方分箱对特征优化分箱，返回分箱信息"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义一个卡方分箱（可设置参数置信度水平与箱的个数）停止条件为大于置信水平且小于bin的数目\n",
    "def ChiMerge(df, variable, flag, confidenceVal=3.841, bin=10, sample = None): \n",
    "    '''\n",
    "    df:传入一个数据框仅包含一个需要卡方分箱的变量与正负样本标识（正样本为1，负样本为0）\n",
    "    variable:需要卡方分箱的变量名称（字符串）\n",
    "    flag：正负样本标识的名称（字符串）\n",
    "    confidenceVal：置信度水平（默认是不进行抽样95%）\n",
    "    bin：最多箱的数目\n",
    "    sample: 为抽样的数目（默认是不进行抽样），因为如果观测值过多运行会较慢\n",
    "    -----------------------\n",
    "    return:\n",
    "    result_data: dataframe\n",
    "    包含变量名，分割间隔，每个间隔对应目标特征的数量\n",
    "    \n",
    "    '''\n",
    "    # 进行是否抽样操作\n",
    "    if sample != None:\n",
    "        df = df.sample(n=sample)\n",
    "    else:\n",
    "        df   \n",
    "\n",
    "    # 进行数据格式化录入\n",
    "    \n",
    "    # 统计需分箱变量每个值的数量\n",
    "    total_num = df.groupby([variable])[flag].count()\n",
    "    \n",
    "    # 转换为dataframe形式\n",
    "    total_num = pd.DataFrame({'total_num': total_num})\n",
    "    \n",
    "    # 统计需分箱变量每个值正样本数量\n",
    "    positive_class = df.groupby([variable])[flag].sum()\n",
    "    \n",
    "    # 转换为dataframe形式\n",
    "    positive_class = pd.DataFrame({'positive_class': positive_class})\n",
    "    \n",
    "    # 组合total_num与positive_class,DataFrame格式\n",
    "    regroup = pd.merge(total_num, positive_class, left_index=True, right_index=True,\n",
    "                       how='inner')\n",
    "    \n",
    "    # 增加索引列\n",
    "    regroup.reset_index(inplace=True)\n",
    "    \n",
    "    # 增加列：分箱变量每个值负样本数量\n",
    "    regroup['negative_class'] = regroup['total_num'] - regroup['positive_class'] \n",
    "    \n",
    "    # 删除 'total_num' 列\n",
    "    regroup = regroup.drop('total_num', axis=1)\n",
    "    \n",
    "    # 把数据框转化为numpy（提高运行效率）\n",
    "    np_regroup = np.array(regroup)  \n",
    "\n",
    "    # 处理连续没有正样本或负样本的区间，并进行区间的合并（以免卡方值计算报错）\n",
    "    i = 0\n",
    "    while (i <= np_regroup.shape[0] - 2):\n",
    "        \n",
    "        # 如果上下两行的正样本个数为0或负样本个数为0\n",
    "        if ((np_regroup[i, 1] == 0 and np_regroup[i + 1, 1] == 0) or ( np_regroup[i, 2] == 0 and np_regroup[i + 1, 2] == 0)):\n",
    "            np_regroup[i, 1] = np_regroup[i, 1] + np_regroup[i + 1, 1]  # 正样本\n",
    "            np_regroup[i, 2] = np_regroup[i, 2] + np_regroup[i + 1, 2]  # 负样本\n",
    "            np_regroup[i, 0] = np_regroup[i + 1, 0]\n",
    "            \n",
    "            # 删除第二行\n",
    "            np_regroup = np.delete(np_regroup, i + 1, 0)\n",
    "            i = i - 1\n",
    "        i = i + 1\n",
    " \n",
    "    # 对相邻两个区间进行卡方值计算\n",
    "    chi_table = np.array([])  # 创建一个数组保存相邻两个区间的卡方值\n",
    "    for i in np.arange(np_regroup.shape[0] - 1):\n",
    "        chi = (np_regroup[i, 1] * np_regroup[i + 1, 2] - np_regroup[i, 2] * np_regroup[i + 1, 1]) ** 2 \\\n",
    "          * (np_regroup[i, 1] + np_regroup[i, 2] + np_regroup[i + 1, 1] + np_regroup[i + 1, 2]) / \\\n",
    "          ((np_regroup[i, 1] + np_regroup[i, 2]) * (np_regroup[i + 1, 1] + np_regroup[i + 1, 2]) * (\n",
    "          np_regroup[i, 1] + np_regroup[i + 1, 1]) * (np_regroup[i, 2] + np_regroup[i + 1, 2]))\n",
    "        chi_table = np.append(chi_table, chi)\n",
    "\n",
    "    # 把卡方值最小的两个区间进行合并（卡方分箱核心）\n",
    "    while (1):     \n",
    "        # 结束循环条件\n",
    "        if (len(chi_table) <= (bin - 1) and min(chi_table) >= confidenceVal):\n",
    "            break\n",
    "            \n",
    "        # 找出卡方值最小的位置索引\n",
    "        chi_min_index = np.argwhere(chi_table == min(chi_table))[0]  \n",
    "        np_regroup[chi_min_index, 1] = np_regroup[chi_min_index, 1] + np_regroup[chi_min_index + 1, 1]\n",
    "        np_regroup[chi_min_index, 2] = np_regroup[chi_min_index, 2] + np_regroup[chi_min_index + 1, 2]\n",
    "        np_regroup[chi_min_index, 0] = np_regroup[chi_min_index + 1, 0]\n",
    "        np_regroup = np.delete(np_regroup, chi_min_index + 1, 0)\n",
    "\n",
    "        if (chi_min_index == np_regroup.shape[0] - 1):  # 最小值试最后两个区间的时候\n",
    "            # 计算合并后当前区间与前一个区间的卡方值并替换\n",
    "            chi_table[chi_min_index - 1] = (np_regroup[chi_min_index - 1, 1] * np_regroup[chi_min_index, 2] - np_regroup[chi_min_index - 1, 2] * np_regroup[chi_min_index, 1]) ** 2 \\\n",
    "                                           * (np_regroup[chi_min_index - 1, 1] + np_regroup[chi_min_index - 1, 2] + np_regroup[chi_min_index, 1] + np_regroup[chi_min_index, 2]) / \\\n",
    "                                       ((np_regroup[chi_min_index - 1, 1] + np_regroup[chi_min_index - 1, 2]) * (np_regroup[chi_min_index, 1] + np_regroup[chi_min_index, 2]) * (np_regroup[chi_min_index - 1, 1] + np_regroup[chi_min_index, 1]) * (np_regroup[chi_min_index - 1, 2] + np_regroup[chi_min_index, 2]))\n",
    "            # 删除替换前的卡方值\n",
    "            chi_table = np.delete(chi_table, chi_min_index, axis=0)\n",
    "\n",
    "        else:\n",
    "            # 计算合并后当前区间与前一个区间的卡方值并替换\n",
    "            chi_table[chi_min_index - 1] = (np_regroup[chi_min_index - 1, 1] * np_regroup[chi_min_index, 2] - np_regroup[chi_min_index - 1, 2] * np_regroup[chi_min_index, 1]) ** 2 \\\n",
    "                                       * (np_regroup[chi_min_index - 1, 1] + np_regroup[chi_min_index - 1, 2] + np_regroup[chi_min_index, 1] + np_regroup[chi_min_index, 2]) / \\\n",
    "                                       ((np_regroup[chi_min_index - 1, 1] + np_regroup[chi_min_index - 1, 2]) * (np_regroup[chi_min_index, 1] + np_regroup[chi_min_index, 2]) * (np_regroup[chi_min_index - 1, 1] + np_regroup[chi_min_index, 1]) * (np_regroup[chi_min_index - 1, 2] + np_regroup[chi_min_index, 2]))\n",
    "            # 计算合并后当前区间与后一个区间的卡方值并替换\n",
    "            chi_table[chi_min_index] = (np_regroup[chi_min_index, 1] * np_regroup[chi_min_index + 1, 2] - np_regroup[chi_min_index, 2] * np_regroup[chi_min_index + 1, 1]) ** 2 \\\n",
    "                                       * (np_regroup[chi_min_index, 1] + np_regroup[chi_min_index, 2] + np_regroup[chi_min_index + 1, 1] + np_regroup[chi_min_index + 1, 2]) / \\\n",
    "                                   ((np_regroup[chi_min_index, 1] + np_regroup[chi_min_index, 2]) * (np_regroup[chi_min_index + 1, 1] + np_regroup[chi_min_index + 1, 2]) * (np_regroup[chi_min_index, 1] + np_regroup[chi_min_index + 1, 1]) * (np_regroup[chi_min_index, 2] + np_regroup[chi_min_index + 1, 2]))\n",
    "            # 删除替换前的卡方值\n",
    "            chi_table = np.delete(chi_table, chi_min_index + 1, axis=0)\n",
    "\n",
    "    # 把结果保存成一个数据框\n",
    "    result_data = pd.DataFrame()  # 创建一个保存结果的数据框\n",
    "    result_data['variable'] = [variable] * np_regroup.shape[0]  # 结果表第一列：变量名\n",
    "    list_temp = []\n",
    "    for i in np.arange(np_regroup.shape[0]):\n",
    "        if i == 0:\n",
    "            x = '0' + ',' + str(np_regroup[i, 0])\n",
    "        elif i == np_regroup.shape[0] - 1:\n",
    "            x = str(np_regroup[i - 1, 0])\n",
    "        else:\n",
    "            x = str(np_regroup[i - 1, 0]) + ',' + str(np_regroup[i, 0])\n",
    "        list_temp.append(x)\n",
    "    result_data['interval'] = list_temp  # 结果表第二列：区间\n",
    "    result_data['flag_0'] = np_regroup[:, 2]  # 结果表第三列：负样本数目\n",
    "    result_data['flag_1'] = np_regroup[:, 1]  # 结果表第四列：正样本数目\n",
    "\n",
    "    return result_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 根据分箱信息对指定变量分箱操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chi_square_bin(cleaned_data, bin_features, flag):\n",
    "    '''\n",
    "    cleaned_data: dataframe\n",
    "    已经自动清理完成的数据\n",
    "    bin_features: list\n",
    "    需要分箱操作的特征名\n",
    "    flag: string\n",
    "    目标特征\n",
    "    '''\n",
    "    \n",
    "    # 对每一个特征进行分箱操作\n",
    "    for feature in bin_features:\n",
    "        \n",
    "        # 返回分割信息\n",
    "        result= ChiMerge(cleaned_data,feature, flag, confidenceVal=3.841, bin=10)\n",
    "        \n",
    "        # 得到分割间隔\n",
    "        res = result['interval'].tolist()\n",
    "        eles = []\n",
    "        for ele in res:\n",
    "            tmp = ele.split(\",\")\n",
    "            for e in tmp:\n",
    "                eles.append(float(e))\n",
    "        seen = set()\n",
    "        bins = tuple(x for x in eles if not (x in seen or seen.add(x)))\n",
    "        \n",
    "        # 根据分割间隔把特征转换成枚举类\n",
    "        categories = pd.cut(input_dataframe[feature],bins)\n",
    "        cleaned_data[feature] = categories.cat.codes \n",
    "    return cleaned_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>female</th>\n",
       "      <th>male</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   female  male  Survived  Pclass  Age  SibSp  Parch  Fare  Embarked\n",
       "0       0     1         0       3    3      1      0     0       0.0\n",
       "1       1     0         1       1    6      1      0     8       1.0\n",
       "2       1     0         1       3    3      0      0     3       1.0\n",
       "3       1     0         1       1    6      1      0     7       1.0\n",
       "4       0     1         0       3    6      0      0     3       0.0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bin_features = ['Age','Fare']\n",
    "flag = 'Survived'\n",
    "cleaned_data = chi_square_bin(cleaned_data, bin_features, flag)\n",
    "cleaned_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 根据IV值筛选出重要的特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WOE:\n",
    "    '''\n",
    "    根据IV值筛选出重要性大的特征\n",
    "    输入：自动清理完成的数据集\n",
    "    输出：每个特征的IV值\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        self._WOE_MIN = -20\n",
    "        self._WOE_MAX = 20\n",
    "\n",
    "    # 计算输入数据中每个变量的WOE和IV\n",
    "    def woe(self, X, y, event=1):\n",
    "        '''\n",
    "        X: 2-D numpy array\n",
    "        包含已经离散化的特征\n",
    "        y: 1-D numpy array\n",
    "        目标特征\n",
    "        :event: 目标特征的其中一个值\n",
    "        Return: \n",
    "        -----------\n",
    "        res_iv: np.array\n",
    "            每个特征的IV值\n",
    "        res_woe: np.array\n",
    "            每个特征每个类的WOE值\n",
    "            \n",
    "        '''\n",
    "        # 检查y是否为binary变量\n",
    "        self.check_target_binary(y)\n",
    "        \n",
    "        # 离散化连续型变量 (自动清理阶段应对连续性变量使用优化分箱离散)\n",
    "        X1 = self.feature_discretion(X)\n",
    "        \n",
    "        # 分别储存WOE, IV值\n",
    "        res_woe = []\n",
    "        res_iv = []\n",
    "        \n",
    "        # 对于每一条记录储存WOE, IV值\n",
    "        for i in range(0, X1.shape[-1]):\n",
    "            x = X1[:, i]\n",
    "            woe_dict, iv1 = self.woe_single_x(x, y, event)\n",
    "            res_woe.append(woe_dict)\n",
    "            res_iv.append(iv1)\n",
    "        return np.array(res_woe), np.array(res_iv)\n",
    "\n",
    "    # 对于每一个特征计算WOE和IV值\n",
    "    def woe_single_x(self, x, y, event=1):\n",
    "\n",
    "        # 检查目标特征是否为binary形式\n",
    "        self.check_target_binary(y)\n",
    "        \n",
    "        # 得到目标变量为1和0的数量\n",
    "        event_total, non_event_total = self.count_binary(y, event=event)\n",
    "        \n",
    "        # 特征取不同值个数\n",
    "        x_labels = np.unique(x)\n",
    "        woe_dict = {}\n",
    "        iv = 0\n",
    "        for x1 in x_labels:\n",
    "            y1 = y[np.where(x == x1)[0]]\n",
    "            # 在该取值时目标变量为1和0的数量和比例\n",
    "            event_count, non_event_count = self.count_binary(y1, event=event)\n",
    "            rate_event = 1.0 * event_count / event_total\n",
    "            rate_non_event = 1.0 * non_event_count / non_event_total\n",
    "            if rate_event == 0:\n",
    "                woe1 = self._WOE_MIN\n",
    "            elif rate_non_event == 0:\n",
    "                woe1 = self._WOE_MAX\n",
    "            else:\n",
    "                woe1 = math.log(rate_event / rate_non_event)\n",
    "            woe_dict[x1] = woe1\n",
    "            # 根据WOE值计算IV值\n",
    "            iv += (rate_event - rate_non_event) * woe1\n",
    "        return woe_dict, iv\n",
    "\n",
    "\n",
    "    def count_binary(self, a, event=1):\n",
    "        '''\n",
    "        计算目标特征分别为0或1的个数\n",
    "        '''\n",
    "        event_count = (a == event).sum()\n",
    "        non_event_count = a.shape[-1] - event_count\n",
    "        return event_count, non_event_count\n",
    "\n",
    "    def check_target_binary(self, y):\n",
    "        '''\n",
    "        检查目标特征是否为binary形式，否则抛出异常\n",
    "        -----------\n",
    "        y: 1-D numpy array\n",
    "        '''\n",
    "        y_type = type_of_target(y)\n",
    "        if y_type not in ['binary']:\n",
    "            raise ValueError('目标特征必须为binary类型')\n",
    "\n",
    "    def feature_discretion(self, X):\n",
    "        '''\n",
    "        离散化连续性变量，保持其他类型变量不变\n",
    "        :param X : numpy array\n",
    "        :return: the numpy array \n",
    "            所有连续性变量已离散化\n",
    "        '''\n",
    "        temp = []\n",
    "        for i in range(0, X.shape[-1]):\n",
    "            x = X[:, i]\n",
    "            x_type = type_of_target(x)\n",
    "            if x_type == 'continuous':\n",
    "                x1 = self.discrete(x)\n",
    "                temp.append(x1)\n",
    "            else:\n",
    "                temp.append(x)\n",
    "        return np.array(temp).T\n",
    "\n",
    "    def discrete(self, x):\n",
    "        '''\n",
    "        离散化特征，分成等间距的5个区间\n",
    "        :param x: 1-D numpy array\n",
    "        :return: discreted 1-D numpy array\n",
    "        '''\n",
    "        res = np.array([0] * x.shape[-1], dtype=int)\n",
    "        for i in range(5):\n",
    "            point1 = stats.scoreatpercentile(x, i * 20)\n",
    "            point2 = stats.scoreatpercentile(x, (i + 1) * 20)\n",
    "            x1 = x[np.where((x >= point1) & (x <= point2))]\n",
    "            mask = np.in1d(x, x1)\n",
    "            res[mask] = (i + 1)\n",
    "        return res\n",
    "\n",
    "    @property\n",
    "    def WOE_MIN(self):\n",
    "        return self._WOE_MIN\n",
    "    @WOE_MIN.setter\n",
    "    def WOE_MIN(self, woe_min):\n",
    "        self._WOE_MIN = woe_min\n",
    "    @property\n",
    "    def WOE_MAX(self):\n",
    "        return self._WOE_MAX\n",
    "    @WOE_MAX.setter\n",
    "    def WOE_MAX(self, woe_max):\n",
    "        self._WOE_MAX = woe_max"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 根据IV值保留重要的特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keep_good_features(cleaned_data,target_feature = cleaned_data.columns.values.tolist()[-1],importance = 0.15):\n",
    "    '''\n",
    "    根据IV值保留重要的特征\n",
    "    输入：自动清理完成的数据集\n",
    "    输出：删除不重要特征的数据集\n",
    "    '''\n",
    "    \n",
    "    target = cleaned_data[target_feature].values\n",
    "    \n",
    "    # 构建WOE实例\n",
    "    test = WOE()\n",
    "    \n",
    "    target_data = cleaned_data[target_feature].values\n",
    "    features_data = cleaned_data.drop([target_feature],axis = 1).values\n",
    "    res_woe, res_iv = test.woe(features_data, target_data, event=1)\n",
    "\n",
    "    iv_list = res_iv.tolist()\n",
    "\n",
    "    features = cleaned_data.columns.values.tolist()\n",
    "\n",
    "\n",
    "    fea_iv_dict = dict(zip(features,iv_list))\n",
    "\n",
    "    remain_fea_iv_dict = dict((key, value) for key, value in fea_iv_dict.items() if value > importance)\n",
    "\n",
    "    remainds = []\n",
    "    for key, value in remain_fea_iv_dict.items():\n",
    "        remainds.append(key) \n",
    "\n",
    "    for feature in cleaned_data.columns.values.tolist():\n",
    "        if not feature in remainds:\n",
    "            remaind_data = cleaned_data.drop([feature],axis = 1)\n",
    "            \n",
    "    return remaind_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "remaind_data = keep_good_features(cleaned_data,target_feature = 'Survived')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>female</th>\n",
       "      <th>male</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   female  male  Survived  Pclass  Age  SibSp  Parch  Fare\n",
       "0       0     1         0       3    3      1      0     0\n",
       "1       1     0         1       1    6      1      0     8\n",
       "2       1     0         1       3    3      0      0     3\n",
       "3       1     0         1       1    6      1      0     7\n",
       "4       0     1         0       3    6      0      0     3"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remaind_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 对自动清理的数据集使用LR模型预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8163265306122449\n"
     ]
    }
   ],
   "source": [
    "y = cleaned_data['Survived']\n",
    "X = cleaned_data.drop(['Survived'],axis=1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "classifier = LogisticRegression() \n",
    "classifier.fit(X_train, y_train) \n",
    "prediction = classifier.predict(X_test)\n",
    "accuracy = accuracy_score(y_test,prediction)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. 自动调参 （暂时未装autosklearn）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import autosklearn.classification\n",
    "import sklearn.model_selection\n",
    "import sklearn.datasets\n",
    "import sklearn.metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARNING] [2018-06-07 11:40:20,633:smac.intensification.intensification.Intensifier] Challenger was the same as the current incumbent; Skipping challenger\n",
      "[WARNING] [2018-06-07 11:40:20,633:smac.intensification.intensification.Intensifier] Challenger was the same as the current incumbent; Skipping challenger\n",
      "[WARNING] [2018-06-07 11:43:24,968:smac.intensification.intensification.Intensifier] Challenger was the same as the current incumbent; Skipping challenger\n",
      "[WARNING] [2018-06-07 11:43:24,968:smac.intensification.intensification.Intensifier] Challenger was the same as the current incumbent; Skipping challenger\n",
      "[WARNING] [2018-06-07 11:43:25,294:smac.intensification.intensification.Intensifier] Challenger was the same as the current incumbent; Skipping challenger\n",
      "[WARNING] [2018-06-07 11:43:25,294:smac.intensification.intensification.Intensifier] Challenger was the same as the current incumbent; Skipping challenger\n",
      "[WARNING] [2018-06-07 11:43:43,133:smac.intensification.intensification.Intensifier] Challenger was the same as the current incumbent; Skipping challenger\n",
      "[WARNING] [2018-06-07 11:43:43,133:smac.intensification.intensification.Intensifier] Challenger was the same as the current incumbent; Skipping challenger\n",
      "[WARNING] [2018-06-07 11:44:07,489:smac.intensification.intensification.Intensifier] Challenger was the same as the current incumbent; Skipping challenger\n",
      "[WARNING] [2018-06-07 11:44:07,489:smac.intensification.intensification.Intensifier] Challenger was the same as the current incumbent; Skipping challenger\n",
      "[WARNING] [2018-06-07 11:44:07,939:smac.intensification.intensification.Intensifier] Challenger was the same as the current incumbent; Skipping challenger\n",
      "[WARNING] [2018-06-07 11:44:07,939:smac.intensification.intensification.Intensifier] Challenger was the same as the current incumbent; Skipping challenger\n",
      "[WARNING] [2018-06-07 11:44:45,190:smac.intensification.intensification.Intensifier] Challenger was the same as the current incumbent; Skipping challenger\n",
      "[WARNING] [2018-06-07 11:44:45,190:smac.intensification.intensification.Intensifier] Challenger was the same as the current incumbent; Skipping challenger\n",
      "[WARNING] [2018-06-07 11:44:53,482:smac.intensification.intensification.Intensifier] Challenger was the same as the current incumbent; Skipping challenger\n",
      "[WARNING] [2018-06-07 11:44:53,482:smac.intensification.intensification.Intensifier] Challenger was the same as the current incumbent; Skipping challenger\n",
      "[WARNING] [2018-06-07 11:44:56,014:smac.intensification.intensification.Intensifier] Challenger was the same as the current incumbent; Skipping challenger\n",
      "[WARNING] [2018-06-07 11:44:56,014:smac.intensification.intensification.Intensifier] Challenger was the same as the current incumbent; Skipping challenger\n",
      "[WARNING] [2018-06-07 11:44:56,312:smac.intensification.intensification.Intensifier] Challenger was the same as the current incumbent; Skipping challenger\n",
      "[WARNING] [2018-06-07 11:44:56,312:smac.intensification.intensification.Intensifier] Challenger was the same as the current incumbent; Skipping challenger\n",
      "[WARNING] [2018-06-07 11:45:08,432:smac.intensification.intensification.Intensifier] Challenger was the same as the current incumbent; Skipping challenger\n",
      "[WARNING] [2018-06-07 11:45:08,432:smac.intensification.intensification.Intensifier] Challenger was the same as the current incumbent; Skipping challenger\n",
      "[WARNING] [2018-06-07 11:45:08,743:smac.intensification.intensification.Intensifier] Challenger was the same as the current incumbent; Skipping challenger\n",
      "[WARNING] [2018-06-07 11:45:08,743:smac.intensification.intensification.Intensifier] Challenger was the same as the current incumbent; Skipping challenger\n",
      "[WARNING] [2018-06-07 11:45:19,282:smac.intensification.intensification.Intensifier] Challenger was the same as the current incumbent; Skipping challenger\n",
      "[WARNING] [2018-06-07 11:45:19,282:smac.intensification.intensification.Intensifier] Challenger was the same as the current incumbent; Skipping challenger\n",
      "[WARNING] [2018-06-07 11:45:22,225:smac.intensification.intensification.Intensifier] Challenger was the same as the current incumbent; Skipping challenger\n",
      "[WARNING] [2018-06-07 11:45:22,225:smac.intensification.intensification.Intensifier] Challenger was the same as the current incumbent; Skipping challenger\n",
      "[WARNING] [2018-06-07 11:45:47,976:smac.intensification.intensification.Intensifier] Challenger was the same as the current incumbent; Skipping challenger\n",
      "[WARNING] [2018-06-07 11:45:47,976:smac.intensification.intensification.Intensifier] Challenger was the same as the current incumbent; Skipping challenger\n",
      "[WARNING] [2018-06-07 11:45:48,248:smac.intensification.intensification.Intensifier] Challenger was the same as the current incumbent; Skipping challenger\n",
      "[WARNING] [2018-06-07 11:45:48,248:smac.intensification.intensification.Intensifier] Challenger was the same as the current incumbent; Skipping challenger\n",
      "[WARNING] [2018-06-07 11:46:00,915:smac.intensification.intensification.Intensifier] Challenger was the same as the current incumbent; Skipping challenger\n",
      "[WARNING] [2018-06-07 11:46:00,915:smac.intensification.intensification.Intensifier] Challenger was the same as the current incumbent; Skipping challenger\n",
      "[WARNING] [2018-06-07 11:46:06,735:smac.intensification.intensification.Intensifier] Challenger was the same as the current incumbent; Skipping challenger\n",
      "[WARNING] [2018-06-07 11:46:06,735:smac.intensification.intensification.Intensifier] Challenger was the same as the current incumbent; Skipping challenger\n",
      "[WARNING] [2018-06-07 11:46:31,334:smac.intensification.intensification.Intensifier] Challenger was the same as the current incumbent; Skipping challenger\n",
      "[WARNING] [2018-06-07 11:46:31,334:smac.intensification.intensification.Intensifier] Challenger was the same as the current incumbent; Skipping challenger\n",
      "[WARNING] [2018-06-07 11:46:31,784:smac.intensification.intensification.Intensifier] Challenger was the same as the current incumbent; Skipping challenger\n",
      "[WARNING] [2018-06-07 11:46:31,784:smac.intensification.intensification.Intensifier] Challenger was the same as the current incumbent; Skipping challenger\n",
      "[WARNING] [2018-06-07 11:51:13,989:smac.intensification.intensification.Intensifier] Challenger was the same as the current incumbent; Skipping challenger\n",
      "[WARNING] [2018-06-07 11:51:13,989:smac.intensification.intensification.Intensifier] Challenger was the same as the current incumbent; Skipping challenger\n",
      "[WARNING] [2018-06-07 11:51:14,526:smac.intensification.intensification.Intensifier] Challenger was the same as the current incumbent; Skipping challenger\n",
      "[WARNING] [2018-06-07 11:51:14,526:smac.intensification.intensification.Intensifier] Challenger was the same as the current incumbent; Skipping challenger\n"
     ]
    }
   ],
   "source": [
    "automl = autosklearn.classification.AutoSklearnClassifier()\n",
    "automl.fit(X_train, y_train)\n",
    "y_hat = automl.predict(X_test)\n",
    "\n",
    "# 得到调参完成后的准确率\n",
    "print(\"Accuracy score\", sklearn.metrics.accuracy_score(y_test, y_hat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ensemble model 中每个模型的权重\n",
    "automl.get_models_with_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ensemble model 中每个模型的参数选择\n",
    "automl.show_models()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
